# This is "example 4", in the section "Examples on the two-node McGill backend"
# in MANUAL.md.  
#
# The yaml file is the same as example 3, except that the number of beams has
# been decreased from 16 to 8.


# This is a production-scale example which will monopolize both 20-core nodes
# frb-compute-0 (which acts as the L0 simulator) and frb-compute-1 (which acts
# as the L1 server).  The L1 server will dedisperse 8 beams, with 16384 frequency
# channels.
#
# nt_per_packet=16 is important here, since the 'fast' kernels are hardcoded to
# use nt_per_packet=16.  See MANUAL.md for more dicussion!

nbeams: 8
nfreq: 16384
nt_per_packet: 16
fpga_counts_per_sample: 384

# This example assumes the nodes are in a non-link-bonded configuration, where
# each of the four 1 Gbps NIC's is on an independent /24 network.  We use UDP
# port 6677 on two NIC's.  Traffic is divided between NIC's "per beam", 
# i.e. four beams will be sent to each NIC.  All NIC's use TCP port 5555 for
# their RPC server.  (Note that in the 8-beam server, only two out of four
# NIC's in the node are used.)

ipaddr: [ "10.2.1.101", "10.2.2.101" ]
port: 6677

rpc_address: [ "tcp://10.2.1.101:5555", 
	       "tcp://10.2.2.101:5555" ]

# The L1 node is configured so that it "thinks" the NFS server is two different
# servers, mounted on directories /frb-archiver1, /frb-archiver2.  File writes to 
# either of these four filesystems will be sent from the corresponding NIC.  The 
# node is responsible for load-balancing file writes between these filesystems.
#
# Here, we define 5 output_devices, corresponding to (2 NIC's) + (1 SSD).
# Each output device will get a separate file I/O thread.  This allows file
# I/O on each device to proceed independently.

output_devices: [ "/ssd", "/frb-archiver-1", "/frb-archiver-2", "/local" ]


# Need to use fast kernels in this example (slow kernels are too slow.)
slow_kernels: False


# Buffer configuration.  For documentation on these parameters, see 
# "Config file reference: L1 server" in MANUAL.md.
#
# Note that the size of the telescoping_ringbuf and write_staging_area
# have been increased relative to example 3.  This is because with 8 beams/node, 
# we have more memory available per beam.

assembled_ringbuf_nsamples: 10000
telescoping_ringbuf_nsamples: [ 120000, 120000, 120000 ]
write_staging_area_gb: 96.0
# For quicker startup:
# telescoping_ringbuf_nsamples: [ 10000, 10000, 10000 ]
# write_staging_area_gb: 1.0

# L1b configuration.
#
# Postprocess triggers using 'toy-l1b.py', a placeholder version
# of the L1b code which "processes" coarse-grained triggers by
# making a big waterfall plot (toy_l1b_beam*.png).
#
# For a production-scale example, it makes sense to set l1b_pipe_timeout=0,
# and set l1b_buffer_nsamples based on the maximum acceptable latency between
# L1a and L1b (in this case we use 4000, corresponding to 4 seconds).  See
# MANUAL.md for discussion!

l1b_executable_filename: "./toy-l1b.py"
l1b_buffer_nsamples: 4000
l1b_pipe_timeout: 0


# stream_acqname.
#
# If the line below is uncommented, the node will continuously stream all incoming data
# to its local SSD.  The stream_acqname should be a unique identifying string which is
# not the same as any existing acquisition on the node.  The data will be written to
# /local/acq_data/$(stream_acqname).

# stream_acqname: "test_acq"
